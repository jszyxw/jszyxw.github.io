<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"nozora.xyz","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="PKU WebMining 2020fall project Boolean-Question 0. 项目结构总览 总体模型结构如下图所示：  其中，图的左侧为输入的文章标题、内容与人类提出的自然问题；图的中部为我们小组所设计的 Pipeline 结构；输入经过我们所设计的 Pipeline 结构，将输出由开放式评测模型生成的模型结果。 其中我们的 Pipeline 结构，根据评测方式分为两部分：">
<meta property="og:type" content="article">
<meta property="og:title" content="BoolQ Models">
<meta property="og:url" content="http://nozora.xyz/boolqModels/index.html">
<meta property="og:site_name" content="ノゾラのブログ">
<meta property="og:description" content="PKU WebMining 2020fall project Boolean-Question 0. 项目结构总览 总体模型结构如下图所示：  其中，图的左侧为输入的文章标题、内容与人类提出的自然问题；图的中部为我们小组所设计的 Pipeline 结构；输入经过我们所设计的 Pipeline 结构，将输出由开放式评测模型生成的模型结果。 其中我们的 Pipeline 结构，根据评测方式分为两部分：">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://nozora.xyz/boolqModels/hw3-pipeline.svg">
<meta property="og:image" content="http://nozora.xyz/boolqModels/微信截图_20210103190118.png">
<meta property="og:image" content="http://nozora.xyz/boolqModels/QG-1609690356199.svg">
<meta property="og:image" content="http://nozora.xyz/boolqModels/image-20210103224559605.png">
<meta property="og:image" content="http://nozora.xyz/boolqModels/image-20210103193431007-1609690349900.png">
<meta property="og:image" content="http://nozora.xyz/boolqModels/open-pipeline.svg">
<meta property="og:image" content="http://nozora.xyz/boolqModels/image-20210103213541471.png">
<meta property="og:image" content="http://nozora.xyz/boolqModels/Spring%20Boot源码初步理解%20(1).svg">
<meta property="og:image" content="http://nozora.xyz/boolqModels/image-20210103221053550.png">
<meta property="og:image" content="http://nozora.xyz/boolqModels/image-20210103221645484.png">
<meta property="article:published_time" content="2021-01-02T16:00:00.000Z">
<meta property="article:modified_time" content="2021-01-02T16:00:00.000Z">
<meta property="article:author" content="nozora">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="reports">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://nozora.xyz/boolqModels/hw3-pipeline.svg">

<link rel="canonical" href="http://nozora.xyz/boolqModels/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>BoolQ Models | ノゾラのブログ</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ノゾラのブログ</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">「漂泊無定之物的棲息地」</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://nozora.xyz/boolqModels/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.ico">
      <meta itemprop="name" content="nozora">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ノゾラのブログ">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BoolQ Models
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-01-03T00:00:00+08:00">2021-01-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="pku-webmining-2020fall-project-boolean-question">PKU WebMining 2020fall project Boolean-Question</h1>
<h2 id="项目结构总览">0. 项目结构总览</h2>
<p>总体模型结构如下图所示：</p>
<p><img src="hw3-pipeline.svg" alt="hw3-pipeline" style="zoom:33%;"></p>
<p>其中，图的左侧为输入的文章标题、内容与人类提出的自然问题；图的中部为我们小组所设计的 Pipeline 结构；输入经过我们所设计的 Pipeline 结构，将输出由开放式评测模型生成的模型结果。</p>
<p>其中我们的 Pipeline 结构，根据评测方式分为两部分：封闭式测试模型与开放式测试模型。</p>
<p>封闭式测试模型架构为 <code>bidirestional LSTM with attention</code>，详见 <code>2.2</code> 节。</p>
<p>开放式测试模型架构为多 <span class="math inline">\(Transformer\)</span> 的集成学习模型，详见第 <span class="math inline">\(3\)</span> 章。</p>
<p>无论是封闭式测试模型还是开放式测试模型，输入数据都需要经过预处理，并且我们在两个模型上进行了针对训练集的数据增强实验。</p>
<p>项目代码地址请见 https://github.com/jszyxw/boolq-project.</p>
<h2 id="数据增强">1. 数据增强</h2>
<p>我们使用了两种方式尝试数据增强，然而两种方式均需要不同程度的手工成本，因此我们仅进行了少量数据的添加，在后面我们默认使用数据增强后的数据集。</p>
<p><img src="微信截图_20210103190118.png" style="zoom:33%;"></p>
<h3 id="基于问题生成的数据增强">1.1 基于问题生成的数据增强</h3>
<p><img src="QG-1609690356199.svg" style="zoom:80%;"></p>
<p>基于问题生成的数据增强如上图所示。训练一个问题生成模型，输入文章、标题、问题答案，输出一个问题以及其置信度。</p>
<p>处于方便，我们直接使用了<a href="%5Bgenerate_boolean_questions_using_T5_transformer%5D(https://github.com/ramsrigouthamg/generate_boolean_questions_using_T5_transformer)">开源代码</a>在 <code>boolQ</code><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> 数据集上进行模型训练，训练后的模型样例输入输出如下：</p>
<p><code>Context</code>:</p>
<blockquote>
<p>Months earlier, Coca-Cola had begun “Project Kansas.” It sounds like a nuclear experiment but it was just a testing project for the new flavor. In individual surveys, they’d found that more than 75% of respondents loved the taste, 15% were indifferent, and 10% had a strong aversion to the taste to the point that they were angry.</p>
</blockquote>
<p><code>Most accurate questions</code>:</p>
<blockquote>
<p>Does coca cola have a kansas flavor? Is project kansas the same as coca cola? Is project kansas a new coca cola flavor?</p>
</blockquote>
<p>然而由于 <code>boolQ</code> 数据集训练量较少（~10k) ，问题生成的质量并不高，需要人工鉴别出符合文章内容的问题，并确认答案是否正确。我们人工检查确认了 1000 多条数据，清洗出 500 个新问题数据，新问题所对应的文章均取自于训练集。</p>
<h3 id="基于问题扰动的数据增强3">1.2 基于问题扰动的数据增强 <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></h3>
<p><img src="image-20210103224559605.png" alt="image-20210103224559605" style="zoom: 33%;"></p>
<p>方法如上图所示，对于一个问题的某个部分进行一定程度上的改变，同时要求答案也尽量改变，通过这种方法构造数据以提高模型的鲁棒性，具体参考论文 <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>。我们尝试使用这种方法生成了 200 条新数据。然而使用这种方法进行数据扩充，对模型的推理能力要求更高，在低资源的封闭式测试中，加入新数据进行模型训练效果反而会变差。</p>
<h2 id="封闭式">2. 封闭式</h2>
<h3 id="预处理">2.1 预处理</h3>
<ol type="1">
<li><p>将原始的 <code>jsonl</code> 文件处理成可训练的 sample</p>
<ol type="1">
<li>输入为 <code>question $ (title) passage</code> 的格式，标签为 <code>answer</code></li>
<li>统一转化为小写</li>
<li>除去<code>数字</code>、<code>字母</code>以及用来标记输入格式的 <code>(</code>，<code>)</code>，<code>$</code> 以外的字符</li>
<li>动词还原为基本形态</li>
<li>删除属于停用词表的词</li>
</ol>
<p>示例如下表：</p></li>
</ol>
<table>
<colgroup>
<col style="width: 7%">
<col style="width: 92%">
</colgroup>
<thead>
<tr class="header">
<th>Attri</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>INPUT</td>
<td>['iran', 'afghanistan', 'speak', 'language', '$', '(persian', 'language)', 'persian', '(', 'p', 'r', 'n', 'n', ')', 'also', 'know', 'endonym', 'farsi', '(', 'f', 'rsi', '(f', 'si', ')', '(', 'listen))', 'one', 'western', 'iranian', 'languages', 'within', 'indo', 'iranian', 'branch', 'indo', 'european', 'language', 'family', 'primarily', 'speak', 'iran', 'afghanistan', '(officially', 'know', 'dari', 'since', ')', 'tajikistan', '(officially', 'know', 'tajiki', 'since', 'soviet', 'era)', 'regions', 'historically', 'persianate', 'societies', 'consider', 'part', 'greater', 'iran', 'write', 'persian', 'alphabet', 'modify', 'variant', 'arabic', 'script', 'evolve', 'aramaic', 'alphabet']</td>
</tr>
<tr class="even">
<td>Label</td>
<td>True</td>
</tr>
</tbody>
</table>
<ol start="2" type="1">
<li> 用预训练词向量 <code>GloVe</code> 做 <code>word embedding</code>。经过实验测试，超参数设为 <span class="math inline">\(dim=100\)</span> 时效果较好。</li>
</ol>
<h3 id="模型搭建">2.2 模型搭建</h3>
<h3 id="模型架构">2.2.1 模型架构</h3>
<p>模型架构为 <code>bidirestional LSTM with attention</code>，参考了 <span class="math inline">\(ACL2016\)</span> 的一篇文章 <code>Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</code><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>，在当时与封闭集评测条件类似的条件下，获得了当时 state of the art 的结果，模型整体结构如下：</p>
<p><img src="image-20210103193431007-1609690349900.png" style="zoom: 50%;"></p>
<h3 id="变种-lstm">2.2.2 变种 LSTM</h3>
<p>参考论文 <a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>，模型中的 LSTM 为变种 LSTM，其主要思想为：在原 LSTM 的基础上，<strong>各个门也将上一个记忆单元考虑上</strong>。修改后各门的具体计算方式如下： <span class="math display">\[
\begin{aligned} i_{t} &amp;=\sigma\left(W_{x i} x_{t}+W_{h i} h_{t-1}+W_{c i} c_{t-1}+b_{i}\right) \\ f_{t} &amp;=\sigma\left(W_{x f} x_{t}+W_{h f} h_{t-1}+W_{c f} c_{t-1}+b_{f}\right) \\ g_{t} &amp;=\tanh \left(W_{x c} x_{t}+W_{h c} h_{t-1}+W_{c c} c_{t-1}+b_{c}\right) \\ c_{t} &amp;=i_{t} g_{t}+f_{t} c_{t-1} \\ o_{t} &amp;=\sigma\left(W_{x o} x_{t}+W_{h o} h_{t-1}+W_{c o} c_{t}+b_{o}\right) \\ h_{t} &amp;=o_{t} \tanh \left(c_{t}\right) \end{aligned}
\]</span></p>
<h3 id="attention-layer">2.2.3 Attention Layer</h3>
<p>Attention 层按如下公式计算： <span class="math display">\[
M =\tanh (H)\\ \alpha =\operatorname{softmax}\left(w^{T} M\right) \\ r =H \alpha^{T} \\ h^{*} =\tanh (r)
\]</span> 其中，H 是 <span class="math inline">\(BiLSTM\)</span> 的输出，H 首先通过 <span class="math inline">\(tanh\)</span> 函数激活得到 M, 再通过 全连接层 + softmax 层 得到。然后 H 乘以权重，得到输出 r 。最后经过 <span class="math inline">\(tanh\)</span> 函数激活得到最后输出 h。得到输出后，直接作为 softmax 层的输入，就可以得到相应预测标签的输出。</p>
<h3 id="训练过程">2.3 训练过程</h3>
<p>训练过程中尝试为微调了 <code>embedding dimension</code>（见 <code>2.1</code> 节）及下表中参数，其中 <code>dropout prob</code>、<code>batch size</code>、<code>lr</code> 对最终结果的稳定性影响较大，其余参数对训练结果影响不大。最终结果大部分均能在 10 个 <code>epoch</code> 以内稳定在 <span class="math inline">\(accuracy &gt; 65%\)</span>, <span class="math inline">\(f1 \approx 50\%\)</span>。</p>
<p>微调后的参数如下：</p>
<table>
<thead>
<tr class="header">
<th>参数</th>
<th>参数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>隐藏层维度</td>
<td> 200</td>
</tr>
<tr class="even">
<td> 注意力层维度</td>
<td> 120</td>
</tr>
<tr class="odd">
<td>LSTM 层数</td>
<td> 2</td>
</tr>
<tr class="even">
<td>Dropout probabiliy</td>
<td>0.5</td>
</tr>
<tr class="odd">
<td>batch size</td>
<td>128</td>
</tr>
<tr class="even">
<td>epoch</td>
<td>20，保存验证集 accuracy 最高的模型参数</td>
</tr>
<tr class="odd">
<td> loss</td>
<td><span class="math inline">\(BCEloss\)</span></td>
</tr>
<tr class="even">
<td>optimizer</td>
<td><span class="math inline">\(Adam(lr=0.001, betas=(0.9, 0.999))\)</span></td>
</tr>
</tbody>
</table>
<h3 id="训练结果">2.4 训练结果</h3>
<p>训练集与验证集结果如下表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>accuracy</th>
<th>F1-score</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 训练集</td>
<td> 98.621</td>
<td>98.158</td>
<td>98.858</td>
<td>97.467</td>
</tr>
<tr class="even">
<td> 验证集</td>
<td> 67.003</td>
<td>51.331</td>
<td>58.061</td>
<td>45.998</td>
</tr>
</tbody>
</table>
<p>测试集结果：见 <code>result/close_result.txt</code></p>
<p>上表中的训练集未加入进行问题扰动型数据增强的数据，训练集加入问题扰动型数据增强的数据后，模型训练结果明显变差，原因可能是 LSTM 模型无法承担如此复杂的、带有推理性质的语法解析任务；因此，基于 Transformer 结构的模型、图神经网络等具有更强语法解析能力、更具推理能力的模型可能对问题扰动更加有效。</p>
<h2 id="开放式">3. 开放式</h2>
<h3 id="预处理-1">3.1 预处理</h3>
<p>将原始的 <code>jsonl</code> 文件导入，并将标题信息融入到文章中，详见 <code>/code/Preprocese.py</code>:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> 
    x <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>line<span class="token punctuation">)</span>
    x<span class="token punctuation">[</span><span class="token string">'passage'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"("</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token string">'title'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">") "</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token string">'passage'</span><span class="token punctuation">]</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="模型结构">3.2 模型结构</h3>
<p>整体模型结构如下图所示：</p>
<p><img src="open-pipeline.svg"></p>
<h3 id="albert-模型">3.2.1 Albert 模型</h3>
<h4 id="模型结构-1">模型结构</h4>
<p>相比于 BERT 模型，ALBERT [^7] 使用了如下 3 个 Trick：</p>
<ul>
<li>对于 word Embedding 进行了因数分解，将单词先投影到低维的 embedding 空间 E，再投影到高维的隐藏空间 H，使得 E 的维度可以不与 H 绑定，大大减少了 embedding 矩阵的的维度（从 <span class="math inline">\(V*H\)</span> 减少至 <span class="math inline">\(V*E+E*H\)</span>）</li>
<li>参数共享，让每一层都使用相同的参数，这样可以大幅度减少参数量（最重要的减少参数量方式），但实际上我们可以看出，虽然谈论的是轻量级，论文中也是给出了 xxlarge 的高参数量版本，毕竟低参数实验不好看。(PS by 宁淳：参数共享我认为本身就是一个宽度和深度的权衡问题，真有多 work 我认为也不一定会有太大作用。）</li>
<li>SOP 任务的提出：对于 Bert 所提出的 NSP 任务，很多后续模型都已经证明其的不适用性。主要在于 NSP 选取不同文档的句子，会导致在预测过程中，不一定单纯的预测句子之间的连贯性，还会很大程度受到不同文档间 topic 的影响。而 SOP 任务反例为两个连续句子的逆序，就能解决这一问题。</li>
</ul>
<h4 id="实验结果">实验结果</h4>
<p>我们使用 <code>Albert-xxlarge</code> 预训练模型，对 BoolQ 任务进行进一步调参，具体代码请见 <code>code/train-albert.py</code></p>
<p>训练过程中尝试微调了下表中参数，其中 <code>dropout prob</code>、<code>batch size</code>、<code>lr</code> 对最终结果的稳定性影响较大，其余参数对训练结果影响不大。最终结果大部分均能在 10 个 <code>epoch</code> 以内稳定在 <span class="math inline">\(accuracy &gt; 87%\)</span>, <span class="math inline">\(f1 \approx 90\%\)</span>。</p>
<p>微调后的参数如下：</p>
<table>
<thead>
<tr class="header">
<th>参数</th>
<th>参数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>隐藏层维度</td>
<td> 200</td>
</tr>
<tr class="even">
<td> 注意力层维度</td>
<td> 120</td>
</tr>
<tr class="odd">
<td>LSTM 层数</td>
<td> 2</td>
</tr>
<tr class="even">
<td>Dropout probabiliy</td>
<td>0.1</td>
</tr>
<tr class="odd">
<td>batch size</td>
<td>32</td>
</tr>
<tr class="even">
<td>epoch</td>
<td>15，保存验证集 accuracy 最高的模型参数</td>
</tr>
<tr class="odd">
<td> loss</td>
<td><span class="math inline">\(CEloss\)</span></td>
</tr>
<tr class="even">
<td>optimizer</td>
<td><span class="math inline">\(AdamW, lr=1e-5, eps=1e-8\)</span></td>
</tr>
</tbody>
</table>
<p>验证集上的结果如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>accuracy</th>
<th>F1-score</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 验证集</td>
<td> 87.49</td>
<td>90.01</td>
<td>91.24</td>
<td>88.93</td>
</tr>
</tbody>
</table>
<h3 id="roberta-模型">3.2.2 RoBERTa 模型</h3>
<h4 id="模型结构-2">模型结构</h4>
<p>在原始 Bert 模型的基础上，RoBERTa<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> 通过实验，证明了如下几点：</p>
<ol type="1">
<li>进一步增加预训练数据数量，能够改善模型效果；</li>
<li>延长预训练时间或增加预训练步数，能够改善模型效果；</li>
<li>急剧放大预训练的每个 Batch 的 Batch Size，能够明显改善模型效果；</li>
<li>拿掉预训练任务中的 Next Sentence Prediction 子任务，它不必要存在；</li>
<li>输入文本的动态 Masking 策略有帮助.</li>
</ol>
<h4 id="实验结果-1">实验结果</h4>
<p>我们使用 <code>RoBERTa-large</code> 预训练模型，对 BoolQ 任务进行进一步调参，具体代码请见 <code>code/train-roberta.py</code></p>
<p>训练过程中尝试微调了下表中参数，其中 <code>dropout prob</code>、<code>batch size</code>、<code>lr</code> 对最终结果的稳定性影响较大，其余参数对训练结果影响不大。最终结果大部分均能在 10 个 <code>epoch</code> 以内稳定在 <span class="math inline">\(accuracy &gt; 87%\)</span>, <span class="math inline">\(f1 \approx 90\%\)</span>。</p>
<p>微调后的参数如下：</p>
<table>
<thead>
<tr class="header">
<th>参数</th>
<th>参数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>隐藏层维度</td>
<td> 200</td>
</tr>
<tr class="even">
<td> 注意力层维度</td>
<td> 120</td>
</tr>
<tr class="odd">
<td>LSTM 层数</td>
<td> 2</td>
</tr>
<tr class="even">
<td>Dropout probabiliy</td>
<td>0.1</td>
</tr>
<tr class="odd">
<td>batch size</td>
<td>32</td>
</tr>
<tr class="even">
<td>epoch</td>
<td>15，保存验证集 accuracy 最高的模型参数</td>
</tr>
<tr class="odd">
<td> loss</td>
<td><span class="math inline">\(CEloss\)</span></td>
</tr>
<tr class="even">
<td>optimizer</td>
<td><span class="math inline">\(AdamW, lr=1e-5, eps=1e-8\)</span></td>
</tr>
</tbody>
</table>
<p>验证集上的结果如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>accuracy</th>
<th>F1-score</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 验证集</td>
<td> 86.51</td>
<td>89.14</td>
<td>89.21</td>
<td>89.08</td>
</tr>
</tbody>
</table>
<h3 id="deberta-模型">3.2.3 DeBERTa 模型</h3>
<h4 id="模型结构-3">模型结构</h4>
<p>相比于 BERT 模型，DeBERTa<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> 使用了如下 2 个 Trick：</p>
<ul>
<li><strong>DeBERTa 使用一种分离的注意机制来进行自我注意</strong>。在 BERT 中，输入层中的每个单词都是用一个向量表示的，这个向量是单词（内容）嵌入和位置嵌入的总和，而 DeBERTa 中的每个单词都是用两个向量表示的，这两个向量分别对其内容和位置进行编码，并且单词之间的注意力权重是根据单词的位置和内容来计算的内容和相对位置。这是因为观察到一对词的注意力权重不仅取决于它们的内容，而且取决于它们的相对位置。例如，当单词 “deep” 和 “learning” 相邻出现时，它们之间的依赖性要比出现在不同句子中时强得多。</li>
<li><strong>DeBERTa 在预训练时增强了 BERT 的输出层。在模型预训练过程中，将 BERT 的输出 Softmax 层替换为一个增强的掩码解码器（EMD）来预测被屏蔽的令牌。</strong>这是为了缓解训练前和微调之间的不匹配。在微调时，我们使用一个任务特定的解码器，它将 BERT 输出作为输入并生成任务标签。然而，在预训练时，我们不使用任何特定任务的解码器，而只是通过 Softmax 归一化 BERT 输出（logits）。因此，我们将掩码语言模型（MLM）视为任何微调任务，并添加一个任务特定解码器，该解码器被实现为两层 Transformer 解码器和 Softmax 输出层，用于预训练。</li>
</ul>
<h4 id="实验结果-2">实验结果</h4>
<p>我们使用 <code>DeBERTa-large</code> 预训练模型，对 BoolQ 任务进行进一步调参，具体代码请见 <code>code/train-deberta.py</code></p>
<p>训练过程中尝试微调了下表中参数，其中 <code>dropout prob</code>、<code>batch size</code>、<code>lr</code> 对最终结果的稳定性影响较大，其余参数对训练结果影响不大。最终结果大部分均能在 10 个 <code>epoch</code> 以内稳定在 <span class="math inline">\(accuracy &gt; 87%\)</span>, <span class="math inline">\(f1 \approx 90\%\)</span>。</p>
<p>微调后的参数如下：</p>
<table>
<thead>
<tr class="header">
<th>参数</th>
<th>参数值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>隐藏层维度</td>
<td> 200</td>
</tr>
<tr class="even">
<td> 注意力层维度</td>
<td> 120</td>
</tr>
<tr class="odd">
<td>LSTM 层数</td>
<td> 2</td>
</tr>
<tr class="even">
<td>Dropout probabiliy</td>
<td>0.1</td>
</tr>
<tr class="odd">
<td>batch size</td>
<td>32</td>
</tr>
<tr class="even">
<td>epoch</td>
<td>15，保存验证集 accuracy 最高的模型参数</td>
</tr>
<tr class="odd">
<td> loss</td>
<td><span class="math inline">\(CEloss\)</span></td>
</tr>
<tr class="even">
<td>optimizer</td>
<td><span class="math inline">\(AdamW, lr=1e-5, eps=1e-8\)</span></td>
</tr>
</tbody>
</table>
<p>验证集上的结果如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>accuracy</th>
<th>F1-score</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 验证集</td>
<td> 87.12</td>
<td>89.56</td>
<td>90.34</td>
<td>88.78</td>
</tr>
</tbody>
</table>
<h3 id="t5-模型">3.2.4 T5 模型</h3>
<h4 id="模型结构-4">模型结构</h4>
<p><img src="image-20210103213541471.png" alt="image-20210103213541471" style="zoom:33%;"></p>
<p>T5 想要将所有的 NLP 任务归为一统，以后所有的 NLP 任务，在其模型下就转化为了如何进行合适的文本输入输出。由于模型预训练量极为巨大，各任务之间可以互相迁移学习，因此对于许多低资源的任务 T5 取得了非常好的效果。而其模型中最闪光的一点是：它使用了相对位置的 embedding，让模型对位置更加敏感 <a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>。然而，T5 本身模型所带的参数量也达到了惊人的 <code>11 billion</code> 的规模，我们小组尝试使用 4 块 11GB 显存的 GPU，仅模型装载就超出了显存限制，进行训练调参更是天方夜谭。在本次实验中，我们进行了大量尝试对论文给出的结果进行复现，然而由于算力资源的限制，我们最终都以失败告终，详见附录。最终，T5 模型我们只是用于作为参考，因为参数量过大难以进行训练，只是进行了一个简单的预测集成。</p>
<h4 id="实验结果-3">实验结果</h4>
<p>我们使用 <code>T5-3b</code> 预训练模型预测结果，具体代码请见 <code>code/T5.py</code></p>
<p>验证集上的结果如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>accuracy</th>
<th>F1-score</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 验证集</td>
<td> 84.25</td>
<td>86.71</td>
<td>91.21</td>
<td>82.64</td>
</tr>
</tbody>
</table>
<p>可以发现结果远远不如论文中所声称的 <code>T5-3b</code> 正确率 <span class="math inline">\(89.9\%\)</span>，而仅为 <span class="math inline">\(84.25\%\)</span>，原因见附录。</p>
<h3 id="模型集成">3.2.5 模型集成</h3>
<p>模型集成结构如下图所示。</p>
<p><img src="Spring%20Boot源码初步理解%20(1).svg"></p>
<p>输入为上述 4 个单模型的预测 <code>yes/no</code> 的经过标准化的概率参数 <code>YES Proba</code> 与 <code>NO Proba</code>，共 8 个值。</p>
<p>使用一层全连接层进行预测，详见 <code>code/ensemble.py</code>。</p>
<p>进行调参后，验证集上的结果如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>accuracy</th>
<th>F1-score</th>
<th>precision</th>
<th>recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td> 验证集</td>
<td> 89.02</td>
<td>91.17</td>
<td>91.15</td>
<td>91.20</td>
</tr>
</tbody>
</table>
<p>可以发现正确率为 89.02%，已与人类表现相当 <a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>，在 <code>SUPER GLUE</code> 排行榜上 <code>BoolQ</code> 任务排名前 5.</p>
<p><img src="image-20210103221053550.png" alt="image-20210103221053550" style="zoom: 33%;"></p>
<h2 id="小组分工情况">4. 小组分工情况</h2>
<h2 id="附录">5. 附录</h2>
<h3 id="t5-复现过程">5.1 T5 复现过程</h3>
<ol type="1">
<li><p><code>T5-11b</code> 尝试装载：</p>
<p>使用 <code>huggingface</code> 文档推荐的配置进行 <code>Transformer</code> 的并行化装载：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tokenizer <span class="token operator">=</span> T5Tokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"t5-11b"</span><span class="token punctuation">)</span>
t5 <span class="token operator">=</span> T5ForConditionalGeneration<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"t5-11b"</span><span class="token punctuation">)</span>
device_map <span class="token operator">=</span> <span class="token punctuation">{</span>
            <span class="token number">0</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
             <span class="token number">1</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
             <span class="token number">2</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
             <span class="token number">3</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
t5<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>device_map<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>运行报错，显存溢出：</p>
<p><img src="image-20210103221645484.png" alt="image-20210103221645484"></p></li>
<li><p><code>T5-3b</code> 尝试复现论文结果：</p>
<p>经过大量尝试对论文给出的 <code>T5-3b</code> 的结果进行复现，修改几天代码后仍然正确率仅有 84% 左右，远低于论文所述 89.9%，故向论文作者进行邮件沟通：</p>
<p>得到回复是 HuggingFace 版的 T5 参数和训练方式与原版有区别，即如需获得论文结果，需要从头开始训练与调参：</p>
<p>从零调参对我们小组的计算资源而言非常不现实，故只好作罢。</p></li>
</ol>
<h3 id="参考文献">5.2 参考文献</h3>
<p>[^7 ]: Lan, Zhenzhong, et al. "Albert: A lite bert for self-supervised learning of language representations." <em>arXiv preprint arXiv:1909.11942</em> (2019).</p>
<section class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Clark, Christopher, et al. "BoolQ: Exploring the surprising difficulty of natural yes/no questions." <em>arXiv preprint arXiv:1905.10044</em> (2019).<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Daniel Khashabi, et al. "More Bang for Your Buck: Natural Perturbation for Robust Question Answering." EMNLP’20<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Daniel Khashabi, et al. "More Bang for Your Buck: Natural Perturbation for Robust Question Answering." EMNLP’20<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Zhou, Peng , et al. "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification." Meeting of the Association for Computational Linguistics 2016.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Zhou, Peng , et al. "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification." Meeting of the Association for Computational Linguistics 2016.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." <em>arXiv preprint arXiv:1907.11692</em> (2019).<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>He, Pengcheng, et al. "DeBERTa: Decoding-enhanced BERT with Disentangled Attention." <em>arXiv preprint arXiv:2006.03654</em> (2020).<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." <em>arXiv preprint arXiv:1910.10683</em> (2019).<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Clark, Christopher, et al. "BoolQ: Exploring the surprising difficulty of natural yes/no questions." <em>arXiv preprint arXiv:1905.10044</em> (2019).<a href="#fnref9" class="footnote-back">↩</a></p></li>
</ol>
</section>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/reports/" rel="tag"># reports</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/add%20index%20to%20hexo%20posts/" rel="prev" title="adding index to hexo posts">
      <i class="fa fa-chevron-left"></i> adding index to hexo posts
    </a></div>
      <div class="post-nav-item">
    <a href="/WebDataMiningNotes/" rel="next" title="pku 互联网数据挖掘 2020 fall 知识点整理">
      pku 互联网数据挖掘 2020 fall 知识点整理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#pku-webmining-2020fall-project-boolean-question"><span class="nav-number">1.</span> <span class="nav-text">PKU WebMining 2020fall project Boolean-Question</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88"><span class="nav-number">1.1.</span> <span class="nav-text">0. 项目结构总览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="nav-number">1.2.</span> <span class="nav-text">1. 数据增强</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%97%AE%E9%A2%98%E7%94%9F%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.1 基于问题生成的数据增强</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%97%AE%E9%A2%98%E6%89%B0%E5%8A%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA3"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2 基于问题扰动的数据增强 2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%81%E9%97%AD%E5%BC%8F"><span class="nav-number">1.3.</span> <span class="nav-text">2. 封闭式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.3.1.</span> <span class="nav-text">2.1 预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.2 模型搭建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">1.3.3.</span> <span class="nav-text">2.2.1 模型架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E7%A7%8D-lstm"><span class="nav-number">1.3.4.</span> <span class="nav-text">2.2.2 变种 LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-layer"><span class="nav-number">1.3.5.</span> <span class="nav-text">2.2.3 Attention Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">1.3.6.</span> <span class="nav-text">2.3 训练过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="nav-number">1.3.7.</span> <span class="nav-text">2.4 训练结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%80%E6%94%BE%E5%BC%8F"><span class="nav-number">1.4.</span> <span class="nav-text">3. 开放式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.1 预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.2 模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#albert-%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.2.1 Albert 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-1"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">实验结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#roberta-%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.4.</span> <span class="nav-text">3.2.2 RoBERTa 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-2"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-1"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">实验结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deberta-%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.5.</span> <span class="nav-text">3.2.3 DeBERTa 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-3"><span class="nav-number">1.4.5.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-2"><span class="nav-number">1.4.5.2.</span> <span class="nav-text">实验结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#t5-%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.6.</span> <span class="nav-text">3.2.4 T5 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84-4"><span class="nav-number">1.4.6.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-3"><span class="nav-number">1.4.6.2.</span> <span class="nav-text">实验结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90"><span class="nav-number">1.4.7.</span> <span class="nav-text">3.2.5 模型集成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%84%E5%88%86%E5%B7%A5%E6%83%85%E5%86%B5"><span class="nav-number">1.5.</span> <span class="nav-text">4. 小组分工情况</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">1.6.</span> <span class="nav-text">5. 附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#t5-%E5%A4%8D%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="nav-number">1.6.1.</span> <span class="nav-text">5.1 T5 复现过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">1.6.2.</span> <span class="nav-text">5.2 参考文献</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="nozora"
      src="/images/favicon.ico">
  <p class="site-author-name" itemprop="name">nozora</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2016 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nozora</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
